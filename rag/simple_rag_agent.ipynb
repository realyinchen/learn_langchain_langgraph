{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20494053",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "\n",
    "- Load document\n",
    "- Split document\n",
    "- Store document\n",
    "\n",
    "### Load Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "525ea58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from rich import print as rprint\n",
    "\n",
    "\n",
    "file_path = \"../example_data/A_SURVEY_ON_AGENTIC_RAG.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf6d0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'producer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'pdfTeX-1.40.25'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'creator'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'LaTeX with hyperref'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'creationdate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-02-05T01:26:00+00:00'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'author'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'moddate'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-02-05T01:26:00+00:00'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ptex.fullbanner'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">6.3.5'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'subject'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'trapped'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/False'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'source'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'../example_data/A_SURVEY_ON_AGENTIC_RAG.pdf'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'total_pages'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'page'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'page_label'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'34'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">page_content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'agentic intelligence, these systems introduce capabilities such as dynamic decision-making, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">iterative reasoning, and\\ncollaborative workflows, enabling them to tackle complex, real-world tasks with enhanced </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">precision and adaptability.\\nThis survey explored the evolution of RAG systems, from their initial implementations </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to advanced paradigms like\\nModular RAG, highlighting the contributions and limitations of each. The integration of</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">agents into the RAG pipeline\\nhas emerged as a pivotal development, resulting in Agentic RAG systems that overcome </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">static workflows and limited\\ncontextual adaptability. Applications across healthcare, finance, education, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">creative industries demonstrate the\\ntransformative potential of these systems, showcasing their ability to deliver</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">personalized, real-time, and context-aware\\nsolutions.\\nDespite their promise, Agentic RAG systems face challenges </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that require further research and innovation. Coordination\\ncomplexity in multi-agent architectures, scalability, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and latency issues, as well as ethical considerations, must be\\naddressed to ensure robust and responsible </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">deployment. Additionally, the lack of specialized benchmarks and datasets\\ntailored to evaluate agentic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities poses a significant hurdle. Developing evaluation methodologies that capture\\nthe unique aspects of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Agentic RAG, such as multi-agent collaboration and dynamic adaptability, will be crucial for\\nadvancing the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">field.\\nLooking ahead, the convergence of retrieval-augmented generation and agentic intelligence has the potential</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">to redefine\\nAI’s role in dynamic and complex environments. By addressing these challenges and exploring future </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">directions,\\nresearchers and practitioners can unlock the full potential of Agentic RAG systems, paving the way for</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">transformative\\napplications across industries and domains. As AI systems continue to evolve, Agentic RAG stands as</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a cornerstone for\\ncreating adaptive, context-aware, and impactful solutions that meet the demands of a rapidly </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">changing world.\\nReferences\\n[1] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Xavier Amatriain, and\\nJianfeng Gao. Large language models: A survey, 2024.\\n[2] Aditi Singh. Exploring language </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">models: A comprehensive survey and analysis. In 2023 International Con-\\nference on Research Methodologies in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge Management, Artificial Intelligence and Telecommunication\\nEngineering (RMKMATE), pages 1–4, 2023.\\n[3] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,\\nJunjie </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren,\\nYifan Li, Xinyu </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language\\nmodels, 2024.\\n[4] Sumit </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kumar Dam, Choong Seon Hong, Yu Qiao, and Chaoning Zhang. A complete survey on llm-based ai\\nchatbots, 2024.\\n[5] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Aditi Singh. A survey of ai text-to-image and ai text-to-video generators. In 2023 4th International Conference\\non</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Artificial Intelligence, Robotics and Control (AIRC), pages 32–36, 2023.\\n[6] Aditi Singh, Abul Ehtesham, Gaurav </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kumar Gupta, Nikhil Kumar Chatta, Saket Kumar, and Tala Talaei Khoei.\\nExploring prompt engineering: A systematic </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">review with swot analysis, 2024.\\n[7] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Wang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language models: Principles,\\ntaxonomy, challenges, and open questions. ACM Transactions on Information Systems, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">November 2024.\\n[8] Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N. Ioannidis, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Huzefa Rangwala,\\nand Christos Faloutsos. Agent-g: An agentic framework for graph retrieval augmented generation, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2024.\\n[9] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Wentao\\nZhang, Jie Jiang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">2024.\\n[10] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Callan,\\nand Graham Neubig. Active retrieval augmented generation, 2023.\\n[11] Yikun Han, Chunjiang Liu, and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Pengfei Wang. A comprehensive survey on vector database: Storage and retrieval\\ntechnique, challenge, 2023.\\n[12] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Anthropic. Building effective agents, 2024. https://www.anthropic.com/research/\\nbuilding-effective-agents. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Accessed: February 2, 2025.\\n[13] LangChain. Langgraph workflows tutorial, 2025. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">https://langchain-ai.github.io/langgraph/\\ntutorials/workflows/. Accessed: February 2, 2025.\\n34'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDocument\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmetadata\u001b[0m=\u001b[1m{\u001b[0m\n",
       "        \u001b[32m'producer'\u001b[0m: \u001b[32m'pdfTeX-1.40.25'\u001b[0m,\n",
       "        \u001b[32m'creator'\u001b[0m: \u001b[32m'LaTeX with hyperref'\u001b[0m,\n",
       "        \u001b[32m'creationdate'\u001b[0m: \u001b[32m'2025-02-05T01:26:00+00:00'\u001b[0m,\n",
       "        \u001b[32m'author'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'keywords'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'moddate'\u001b[0m: \u001b[32m'2025-02-05T01:26:00+00:00'\u001b[0m,\n",
       "        \u001b[32m'ptex.fullbanner'\u001b[0m: \u001b[32m'This is pdfTeX, Version 3.141592653-2.6-1.40.25 \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTeX Live 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m kpathsea version \u001b[0m\n",
       "\u001b[32m6.3.5'\u001b[0m,\n",
       "        \u001b[32m'subject'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'title'\u001b[0m: \u001b[32m''\u001b[0m,\n",
       "        \u001b[32m'trapped'\u001b[0m: \u001b[32m'/False'\u001b[0m,\n",
       "        \u001b[32m'source'\u001b[0m: \u001b[32m'../example_data/A_SURVEY_ON_AGENTIC_RAG.pdf'\u001b[0m,\n",
       "        \u001b[32m'total_pages'\u001b[0m: \u001b[1;36m39\u001b[0m,\n",
       "        \u001b[32m'page'\u001b[0m: \u001b[1;36m33\u001b[0m,\n",
       "        \u001b[32m'page_label'\u001b[0m: \u001b[32m'34'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpage_content\u001b[0m=\u001b[32m'agentic intelligence, these systems introduce capabilities such as dynamic decision-making, \u001b[0m\n",
       "\u001b[32miterative reasoning, and\\ncollaborative workflows, enabling them to tackle complex, real-world tasks with enhanced \u001b[0m\n",
       "\u001b[32mprecision and adaptability.\\nThis survey explored the evolution of RAG systems, from their initial implementations \u001b[0m\n",
       "\u001b[32mto advanced paradigms like\\nModular RAG, highlighting the contributions and limitations of each. The integration of\u001b[0m\n",
       "\u001b[32magents into the RAG pipeline\\nhas emerged as a pivotal development, resulting in Agentic RAG systems that overcome \u001b[0m\n",
       "\u001b[32mstatic workflows and limited\\ncontextual adaptability. Applications across healthcare, finance, education, and \u001b[0m\n",
       "\u001b[32mcreative industries demonstrate the\\ntransformative potential of these systems, showcasing their ability to deliver\u001b[0m\n",
       "\u001b[32mpersonalized, real-time, and context-aware\\nsolutions.\\nDespite their promise, Agentic RAG systems face challenges \u001b[0m\n",
       "\u001b[32mthat require further research and innovation. Coordination\\ncomplexity in multi-agent architectures, scalability, \u001b[0m\n",
       "\u001b[32mand latency issues, as well as ethical considerations, must be\\naddressed to ensure robust and responsible \u001b[0m\n",
       "\u001b[32mdeployment. Additionally, the lack of specialized benchmarks and datasets\\ntailored to evaluate agentic \u001b[0m\n",
       "\u001b[32mcapabilities poses a significant hurdle. Developing evaluation methodologies that capture\\nthe unique aspects of \u001b[0m\n",
       "\u001b[32mAgentic RAG, such as multi-agent collaboration and dynamic adaptability, will be crucial for\\nadvancing the \u001b[0m\n",
       "\u001b[32mfield.\\nLooking ahead, the convergence of retrieval-augmented generation and agentic intelligence has the potential\u001b[0m\n",
       "\u001b[32mto redefine\\nAI’s role in dynamic and complex environments. By addressing these challenges and exploring future \u001b[0m\n",
       "\u001b[32mdirections,\\nresearchers and practitioners can unlock the full potential of Agentic RAG systems, paving the way for\u001b[0m\n",
       "\u001b[32mtransformative\\napplications across industries and domains. As AI systems continue to evolve, Agentic RAG stands as\u001b[0m\n",
       "\u001b[32ma cornerstone for\\ncreating adaptive, context-aware, and impactful solutions that meet the demands of a rapidly \u001b[0m\n",
       "\u001b[32mchanging world.\\nReferences\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, \u001b[0m\n",
       "\u001b[32mXavier Amatriain, and\\nJianfeng Gao. Large language models: A survey, 2024.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Aditi Singh. Exploring language \u001b[0m\n",
       "\u001b[32mmodels: A comprehensive survey and analysis. In 2023 International Con-\\nference on Research Methodologies in \u001b[0m\n",
       "\u001b[32mKnowledge Management, Artificial Intelligence and Telecommunication\\nEngineering \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRMKMATE\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, pages 1–4, 2023.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,\\nJunjie \u001b[0m\n",
       "\u001b[32mZhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren,\\nYifan Li, Xinyu \u001b[0m\n",
       "\u001b[32mTang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language\\nmodels, 2024.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m4\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Sumit \u001b[0m\n",
       "\u001b[32mKumar Dam, Choong Seon Hong, Yu Qiao, and Chaoning Zhang. A complete survey on llm-based ai\\nchatbots, 2024.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mAditi Singh. A survey of ai text-to-image and ai text-to-video generators. In 2023 4th International Conference\\non\u001b[0m\n",
       "\u001b[32mArtificial Intelligence, Robotics and Control \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAIRC\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, pages 32–36, 2023.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Aditi Singh, Abul Ehtesham, Gaurav \u001b[0m\n",
       "\u001b[32mKumar Gupta, Nikhil Kumar Chatta, Saket Kumar, and Tala Talaei Khoei.\\nExploring prompt engineering: A systematic \u001b[0m\n",
       "\u001b[32mreview with swot analysis, 2024.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m7\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian \u001b[0m\n",
       "\u001b[32mWang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large \u001b[0m\n",
       "\u001b[32mlanguage models: Principles,\\ntaxonomy, challenges, and open questions. ACM Transactions on Information Systems, \u001b[0m\n",
       "\u001b[32mNovember 2024.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m8\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N. Ioannidis, \u001b[0m\n",
       "\u001b[32mHuzefa Rangwala,\\nand Christos Faloutsos. Agent-g: An agentic framework for graph retrieval augmented generation, \u001b[0m\n",
       "\u001b[32m2024.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, \u001b[0m\n",
       "\u001b[32mWentao\\nZhang, Jie Jiang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey, \u001b[0m\n",
       "\u001b[32m2024.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m10\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie \u001b[0m\n",
       "\u001b[32mCallan,\\nand Graham Neubig. Active retrieval augmented generation, 2023.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m11\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Yikun Han, Chunjiang Liu, and \u001b[0m\n",
       "\u001b[32mPengfei Wang. A comprehensive survey on vector database: Storage and retrieval\\ntechnique, challenge, 2023.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mAnthropic. Building effective agents, 2024. https://www.anthropic.com/research/\\nbuilding-effective-agents. \u001b[0m\n",
       "\u001b[32mAccessed: February 2, 2025.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m13\u001b[0m\u001b[32m]\u001b[0m\u001b[32m LangChain. Langgraph workflows tutorial, 2025. \u001b[0m\n",
       "\u001b[32mhttps://langchain-ai.github.io/langgraph/\\ntutorials/workflows/. Accessed: February 2, 2025.\\n34'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(docs[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988cfb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[33].page_content = docs[33].page_content.split(\"\\nReferences\\n\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1746cd74",
   "metadata": {},
   "source": [
    "### Split Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db8d64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs[:34])\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01895b1",
   "metadata": {},
   "source": [
    "# Store Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fd804f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['76729433a1434c43838efbbb477cd7e7',\n",
       " 'e984b0d2381c436881eb9525475d40c6',\n",
       " '330f5862adf4454db1c4daf5abbe2a70',\n",
       " '1120a2d8ef484208931d6d41f2bcd4a6',\n",
       " '5201fba80bc84864b13476b7aa1f5f83',\n",
       " '090586af2fac4317bdf27664a04d2163',\n",
       " 'afba64f3e9424f20a2c1613b911db486',\n",
       " '8efdf5d43306414a8faf20db65160bda',\n",
       " '1c5472456e074758ae79944e0300d3be',\n",
       " 'f63c58bb6d52407fa31bd554e47699e3',\n",
       " '5ca19172202340cfbb7dd47b817b06e1',\n",
       " '83d9ad4effc14e33ac63c7b528e7ee28',\n",
       " '31abecc4ef21407baba3d8681e7a720b',\n",
       " '363306b5d32845389b0d931b721993aa',\n",
       " '649c7cdef59346919b71ef3637cebee4',\n",
       " 'bbf0b4b02f254adfbeff9228b029b50f',\n",
       " 'd4271129ec74495ba68331fe7d553d3e',\n",
       " '0b0aecf096d84e059a30c23c70965ee7',\n",
       " 'ef93f613ce8e492cbe0dc7243aada0b1',\n",
       " 'd4fe853634ec4180b6a7d48761d4f66d',\n",
       " '85198f82dc004e4393c722f318c949df',\n",
       " '6a159a4927264cecb5c95133222ea131',\n",
       " 'b48ffd1b9d624a538167d1ae63fc7fb7',\n",
       " 'a1ecf2f024224bec9b657d11c69f475c',\n",
       " 'd099d25e0bc949a989437108b05cec22',\n",
       " '59e89991fd4a439ab6c0a6f96e1f0afd',\n",
       " '904cbe0078f047478158e5c77a01c192',\n",
       " '6e38d96688ab44839d793556f1528fdf',\n",
       " '7c79a4a7348b444a984713f903a4697b',\n",
       " '76e78bca71064586bb12ea28f1159e86',\n",
       " 'ec1eb1a10f6a41ce8768e0a7e33f1dc7',\n",
       " '20efaa9c44a7473d9ea0e059f7e2baa0',\n",
       " 'c147c6c3d7fa47afae184c7d2c4277c2',\n",
       " '43a45e246a4a404dbe3b4899f4161a9f',\n",
       " '3209a37c2607462f850c8ed90f8cdf07',\n",
       " 'c809edcee5344c4f8612c325ef982bd3',\n",
       " '9c1a0ae366bb4410a3137ce2e2301202',\n",
       " '5caeec9108b540cc81abdcbd02c07fb5',\n",
       " '7c6f6b802b6f4048a5838862321164fe',\n",
       " '8e86ac193042453cbea091a56576acba',\n",
       " '7159362f38374447be9c2b74f111165c',\n",
       " '63e965182a524936b49ebaa1ca36e676',\n",
       " 'b112cd6ec2eb453199ac4449617fede7',\n",
       " 'd4dccb24dbb44e14816a1515bfe563d9',\n",
       " 'ce3dde80419b4cfbab6889d5434e6459',\n",
       " '0009797aa677445c8391c037bd5885b4',\n",
       " 'd4d02918ac5a47898f1430be6084a500',\n",
       " 'f951fbcfe2e24a5287650f99b31319cd',\n",
       " 'f07713ac70044a53b1acc67991608a17',\n",
       " '93b911e2eadf4e3dae59b8c42433813d',\n",
       " 'cc5ae238c34443988459b623cecbfc05',\n",
       " '48ee9a7bd3a847ce91d277e86a6c7465',\n",
       " 'cd0bae2b971c49f798235e4f59b3ae9a',\n",
       " '32e50a72e36f48f9b9e79b7ad72be0f1',\n",
       " 'fd800baee3a64dd7a29d8a2820c8f14d',\n",
       " 'd47c0348638242e59886e11d6a7d0903',\n",
       " 'f4cf8b334d514abf83fe2b41e6c9e161',\n",
       " 'a055ff4512dd4410971a92602e192e1c',\n",
       " '93b2fa6ee5fc4a7782bf4951bcc7c3f4',\n",
       " '728cb483f23c4746a9b3b5db41af4e11',\n",
       " '7870f86d1a08458984785e9469ceacf0',\n",
       " '58edfa5f3d854cd5adaaf8bb44e5c1ce',\n",
       " '071f08f8693f4382898aa5524491af78',\n",
       " '5a784e5d9ff14f0da9f7af27c037364e',\n",
       " 'e2e2bf48cdce4a4ea54b2199c8ad93d4',\n",
       " '59811c365c1d44dba74ae4d1e8852dff',\n",
       " 'bf1a3c7d05eb48fc9d7a08f0f6eb7879',\n",
       " '0e30dcf4d74146719a18bad5c1efecad',\n",
       " 'dd77d25aed35411eb1e75dec4c5d966d',\n",
       " '3a302bd648a54064a4dfafb29a059ba8',\n",
       " 'a03cda8f568c4c30ab542ece66b860df',\n",
       " '4a4a135fc6184de49ed01a27e1808594',\n",
       " 'c01410133a8a4caabc78845609725def',\n",
       " 'ade6a758338a47ef8c65db51e1c9850e',\n",
       " '2e96029537834cdb892cb54bed5adc05',\n",
       " 'f64b22a9c2774c4f96f1e210f8393270',\n",
       " 'a72fa15b712d4cc1a8a1f4a7157e2576',\n",
       " '197f086215d74cd49f8ad0006e49e2ef',\n",
       " '74b225ffca014fd6a51b9ce29bfd3b5d',\n",
       " 'be7d6ff5fcea4a138c00a56207665905',\n",
       " 'f099bbde1ce44d25895a127c0734211f',\n",
       " '1a83f18fa78d4fb68d8630ea7d6edace',\n",
       " '525c18e6a2ce4420a3adae0c94fdf7b2',\n",
       " '7f2913def17a4bf09430698a1ef864b5',\n",
       " 'a9688197ffec407c91fa0b010183c151',\n",
       " '20d89a5778be46548f01bacce3cd359f',\n",
       " '41b75b39af9046c29ae9f384f61059e6',\n",
       " '27b8618e1dc7443c825014f7a39e9c8a',\n",
       " 'dcfb5e5e2d924d39ab1e79d75f5e7f7a',\n",
       " 'd422b056da4a498aa17b7c999929bc4c',\n",
       " '46409418d2b544f2ac5138af1f9e49ad',\n",
       " '8de8588b321144df90f1cdb5121de693',\n",
       " 'c49f36b3effe41458a54704441ab17aa',\n",
       " '4a92ee3341f74773b066052b16ae644f',\n",
       " '22025af1c0c942878997400073ebdbb5',\n",
       " 'd57c54cd807544a4997d1973775f8f72',\n",
       " '33a477d2d3f64b668dfd7eaa1a110e7a',\n",
       " '3ceee1af973a46418b75ff463f827425',\n",
       " 'd7a7e79e17be4a1aa1adc140298debaf',\n",
       " '08e1744a7d91421b8e70c5116f9937cf',\n",
       " '86cfead7d9a64a629061e0c307768fae',\n",
       " '277ded8076374aad96ce40ffc6e2709b',\n",
       " '8b2f44d890e64997b4d567748061ea24',\n",
       " '1cffc04443b0484f91f0b615b2c6a900',\n",
       " '62398b8f2da84cf4aa13f438a8e46589',\n",
       " '8ed14da0cd464b9d9dc24bc987520f64',\n",
       " '9f06a78c354d49618a4b3f3ae8547fdf',\n",
       " '542dda2ded38426e9fd61a45788f0819',\n",
       " '0e690de5bda24ea691c72129a2d8395c',\n",
       " '5b5cdbf8eb184af8a4414407c4614ed5']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "embedding_model = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v4\",\n",
    "    dashscope_api_key=os.environ.get(\"COMPATIBLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"agentic_rag_survey\",\n",
    "    vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"agentic_rag_survey\",\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "vectorstore.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdbf95e",
   "metadata": {},
   "source": [
    "# Retrieval and Generation\n",
    "\n",
    "- RAG agents\n",
    "- RAG chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775aeffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=os.environ.get(\"MODEL_NAME\"),\n",
    "    temperature=0,\n",
    "    base_url=os.environ.get(\"COMPATIBLE_BASE_URL\"),\n",
    "    api_key=os.environ.get(\"COMPATIBLE_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9894c8f",
   "metadata": {},
   "source": [
    "### RAG agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576727b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd3f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a paper. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7c910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is RAG?\n",
      "Once you get the answer, look up the core components of it?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_8ee3389a93d6432abde680d2)\n",
      " Call ID: call_8ee3389a93d6432abde680d2\n",
      "  Args:\n",
      "    query: What is RAG?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../example_data/A_SURVEY_ON_AGENTIC_RAG.pdf', 'total_pages': 39, 'page': 0, 'page_label': '1', 'start_index': 781, '_id': 'e984b0d2-381c-4368-81eb-9525475d40c6', '_collection_name': 'agentic_rag_survey'}\n",
      "Content: outputs. Retrieval-Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by\n",
      "integrating real-time data retrieval to provide contextually relevant and up-to-date responses. Despite\n",
      "its promise, traditional RAG systems are constrained by static workflows and lack the adaptability\n",
      "required for multi-step reasoning and complex task management.\n",
      "Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding\n",
      "autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflec-\n",
      "tion, planning, tool use, and multi-agent collaboration to dynamically manage retrieval strategies,\n",
      "iteratively refine contextual understanding, and adapt workflows through clearly defined operational\n",
      "structures ranging from sequential steps to adaptive collaboration. This integration enables Agentic\n",
      "RAG systems to deliver unparalleled flexibility, scalability, and context-awareness across diverse\n",
      "applications.\n",
      "\n",
      "Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../example_data/A_SURVEY_ON_AGENTIC_RAG.pdf', 'total_pages': 39, 'page': 6, 'page_label': '7', 'start_index': 0, '_id': '6a159a49-2726-4cec-b5c9-5133222ea131', '_collection_name': 'agentic_rag_survey'}\n",
      "Content: Figure 6: Overview of Graph RAG\n",
      "Key characteristics of Agentic RAG include:\n",
      "• Autonomous Decision-Making: Agents independently evaluate and manage retrieval strategies based on\n",
      "query complexity.\n",
      "• Iterative Refinement: Incorporates feedback loops to improve retrieval accuracy and response relevance.\n",
      "• Workflow Optimization: Dynamically orchestrates tasks, enabling efficiency in real-time applications.\n",
      "Despite its advancements, Agentic RAG faces some challenges:\n",
      "• Coordination Complexity : Managing interactions between agents requires sophisticated orchestration\n",
      "mechanisms.\n",
      "• Computational Overhead: The use of multiple agents increases resource requirements for complex work-\n",
      "flows.\n",
      "• Scalability Limitations: While scalable, the dynamic nature of the system can strain computational resources\n",
      "for high query volumes.\n",
      "Agentic RAG excels in domains like customer support, financial analytics, and adaptive learning platforms, where\n",
      "dynamic adaptability and contextual precision are paramount.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_27993a7973344a9a81413c00)\n",
      " Call ID: call_27993a7973344a9a81413c00\n",
      "  Args:\n",
      "    query: core components of RAG\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../example_data/A_SURVEY_ON_AGENTIC_RAG.pdf', 'total_pages': 39, 'page': 2, 'page_label': '3', 'start_index': 0, '_id': '5ca19172-2023-40cf-bb7d-d47b817b06e1', '_collection_name': 'agentic_rag_survey'}\n",
      "Content: Figure 1: An Overview of Agentic RAG\n",
      "2.2 Core Components of RAG\n",
      "The architecture of RAG systems integrates three primary components (Figure2):\n",
      "• Retrieveal: Responsible for querying external data sources such as knowledge bases, APIs, or vector databases.\n",
      "Advanced retrievers leverage dense vector search and transformer-based models to improve retrieval precision\n",
      "and semantic relevance.\n",
      "• Augmentation: Processes retrieved data, extracting and summarizing the most relevant information to align\n",
      "with the query context.\n",
      "• Generation: Combines retrieved information with the LLM’s pre-trained knowledge to generate coherent,\n",
      "contextually appropriate responses.\n",
      "2.3 Evolution of RAG Paradigms\n",
      "The field of Retrieval-Augmented Generation (RAG) has evolved significantly to address the increasing complexity of\n",
      "real-world applications, where contextual accuracy, scalability, and multi-step reasoning are critical. What began as\n",
      "\n",
      "Source: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../example_data/A_SURVEY_ON_AGENTIC_RAG.pdf', 'total_pages': 39, 'page': 6, 'page_label': '7', 'start_index': 0, '_id': '6a159a49-2726-4cec-b5c9-5133222ea131', '_collection_name': 'agentic_rag_survey'}\n",
      "Content: Figure 6: Overview of Graph RAG\n",
      "Key characteristics of Agentic RAG include:\n",
      "• Autonomous Decision-Making: Agents independently evaluate and manage retrieval strategies based on\n",
      "query complexity.\n",
      "• Iterative Refinement: Incorporates feedback loops to improve retrieval accuracy and response relevance.\n",
      "• Workflow Optimization: Dynamically orchestrates tasks, enabling efficiency in real-time applications.\n",
      "Despite its advancements, Agentic RAG faces some challenges:\n",
      "• Coordination Complexity : Managing interactions between agents requires sophisticated orchestration\n",
      "mechanisms.\n",
      "• Computational Overhead: The use of multiple agents increases resource requirements for complex work-\n",
      "flows.\n",
      "• Scalability Limitations: While scalable, the dynamic nature of the system can strain computational resources\n",
      "for high query volumes.\n",
      "Agentic RAG excels in domains like customer support, financial analytics, and adaptive learning platforms, where\n",
      "dynamic adaptability and contextual precision are paramount.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**What is RAG?**  \n",
      "Retrieval-Augmented Generation (RAG) is a framework designed to enhance Large Language Models (LLMs) by integrating real-time data retrieval. This allows LLMs to provide responses that are not only contextually relevant but also up-to-date by pulling information from external sources like knowledge bases, APIs, or vector databases.\n",
      "\n",
      "---\n",
      "\n",
      "**Core Components of RAG:**  \n",
      "The architecture of RAG systems consists of three primary components:\n",
      "\n",
      "1. **Retrieval**:  \n",
      "   - Queries external data sources (e.g., knowledge bases, APIs, vector databases).  \n",
      "   - Uses advanced techniques like dense vector search and transformer-based models to ensure high precision and semantic relevance in retrieved data.\n",
      "\n",
      "2. **Augmentation**:  \n",
      "   - Processes the retrieved data by extracting and summarizing the most relevant pieces of information.  \n",
      "   - Aligns this information with the context of the user's query.\n",
      "\n",
      "3. **Generation**:  \n",
      "   - Combines the retrieved and processed information with the LLM’s pre-trained knowledge.  \n",
      "   - Produces coherent and contextually appropriate responses.\n",
      "\n",
      "This structured approach enables RAG systems to overcome limitations of static LLM knowledge and better handle dynamic, real-world applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "\n",
    "query = (\n",
    "    \"What is RAG?\\n\"\n",
    "    \"Once you get the answer, look up the core components of it?\"\n",
    ")\n",
    "\n",
    "for event in agent.stream({\"messages\": [HumanMessage(query)]}, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1315c81",
   "metadata": {},
   "source": [
    "### RAG chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cca574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vectorstore.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff56f015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is RAG?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Retrieval-Augmented Generation (RAG)** is a framework that enhances **Large Language Models (LLMs)** by integrating **real-time data retrieval** from external sources—such as knowledge bases, APIs, or the web—into the response generation process.\n",
      "\n",
      "### Why RAG?\n",
      "Traditional LLMs rely solely on their **static training data**, which can lead to:\n",
      "- **Outdated information**\n",
      "- **Hallucinated (factually incorrect) responses**\n",
      "- **Inability to adapt to dynamic or real-world scenarios**\n",
      "\n",
      "RAG addresses these limitations by **retrieving relevant, up-to-date information** at query time and using it to **ground the model’s output in factual, context-specific data**.\n",
      "\n",
      "### How RAG Works (Simplified):\n",
      "1. **Retrieve**: When a user asks a question, the system searches an external knowledge source for relevant documents or data.\n",
      "2. **Augment**: The retrieved information is combined with the original query.\n",
      "3. **Generate**: The LLM uses this enriched context to produce a more accurate, relevant, and timely response.\n",
      "\n",
      "### Benefits of RAG:\n",
      "- Improves **factual accuracy**\n",
      "- Enables access to **current information** beyond the model’s training cutoff\n",
      "- Reduces **hallucinations**\n",
      "- Bridges the gap between **static models** and **dynamic real-world needs**\n",
      "\n",
      "However, **traditional RAG systems** use **linear, static workflows**, limiting their ability to handle **complex reasoning**, **multi-step tasks**, or **adaptive interactions**—which is where advanced approaches like **Agentic RAG** come in.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is RAG?\"\n",
    "for step in agent.stream({\"messages\": [HumanMessage(query)]}, stream_mode=\"values\"):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72211ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain.agents.middleware import AgentMiddleware, AgentState\n",
    "\n",
    "\n",
    "class State(AgentState):\n",
    "    context: list[Document]\n",
    "\n",
    "\n",
    "class RetrieveDocumentsMiddleware(AgentMiddleware[State]):\n",
    "    state_schema = State\n",
    "\n",
    "    def before_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        retrieved_docs = vectorstore.similarity_search(last_message.text)\n",
    "\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "        augmented_message_content = (\n",
    "            f\"{last_message.text}\\n\\n\"\n",
    "            \"Use the following context to answer the query:\\n\"\n",
    "            f\"{docs_content}\"\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\n",
    "            \"context\": retrieved_docs,\n",
    "        }\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=[],\n",
    "    middleware=[RetrieveDocumentsMiddleware()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dce893f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is RAG?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is RAG?\n",
      "\n",
      "Use the following context to answer the query:\n",
      "outputs. Retrieval-Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by\n",
      "integrating real-time data retrieval to provide contextually relevant and up-to-date responses. Despite\n",
      "its promise, traditional RAG systems are constrained by static workflows and lack the adaptability\n",
      "required for multi-step reasoning and complex task management.\n",
      "Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding\n",
      "autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflec-\n",
      "tion, planning, tool use, and multi-agent collaboration to dynamically manage retrieval strategies,\n",
      "iteratively refine contextual understanding, and adapt workflows through clearly defined operational\n",
      "structures ranging from sequential steps to adaptive collaboration. This integration enables Agentic\n",
      "RAG systems to deliver unparalleled flexibility, scalability, and context-awareness across diverse\n",
      "applications.\n",
      "\n",
      "Figure 6: Overview of Graph RAG\n",
      "Key characteristics of Agentic RAG include:\n",
      "• Autonomous Decision-Making: Agents independently evaluate and manage retrieval strategies based on\n",
      "query complexity.\n",
      "• Iterative Refinement: Incorporates feedback loops to improve retrieval accuracy and response relevance.\n",
      "• Workflow Optimization: Dynamically orchestrates tasks, enabling efficiency in real-time applications.\n",
      "Despite its advancements, Agentic RAG faces some challenges:\n",
      "• Coordination Complexity : Managing interactions between agents requires sophisticated orchestration\n",
      "mechanisms.\n",
      "• Computational Overhead: The use of multiple agents increases resource requirements for complex work-\n",
      "flows.\n",
      "• Scalability Limitations: While scalable, the dynamic nature of the system can strain computational resources\n",
      "for high query volumes.\n",
      "Agentic RAG excels in domains like customer support, financial analytics, and adaptive learning platforms, where\n",
      "dynamic adaptability and contextual precision are paramount.\n",
      "\n",
      "• Risk Mitigation: Identifies potential risks using predictive analysis and multi-step reasoning.\n",
      "• Enhanced Decision-Making: Combines historical and live data for comprehensive strategies.\n",
      "7.5 Education and Personalized Learning\n",
      "Education is another domain where Agentic RAG systems are making significant strides. These systems enable adaptive\n",
      "learning by generating explanations, study materials, and feedback tailored to the learner’s progress and preferences.\n",
      "Use Case: Research Paper Generation [41]\n",
      "In higher education, Agentic RAG has been used to assist researchers by synthesizing key findings from multiple\n",
      "sources. For instance, a researcher querying, “What are the latest advancements in quantum computing?” receives a\n",
      "concise summary enriched with references, enhancing the quality and efficiency of their work.\n",
      "Key Benefits:\n",
      "• Tailored Learning Paths: Adapts content to individual student needs and performance levels.\n",
      "\n",
      "reliance often results in outdated information, hallucinated responses [7], and an inability to adapt to dynamic, real-world\n",
      "scenarios. These challenges emphasize the need for systems that can integrate real-time data and dynamically refine\n",
      "responses to maintain contextual relevance and accuracy.\n",
      "Retrieval-Augmented Generation (RAG) [8, 9] emerged as a promising solution to these challenges. By combining\n",
      "the generative capabilities of LLMs with external retrieval mechanisms [10], RAG systems enhance the relevance and\n",
      "timeliness of responses. These systems retrieve real-time information from sources such as knowledge bases [11], APIs,\n",
      "or the web, effectively bridging the gap between static training data and the demands of dynamic applications. However,\n",
      "traditional RAG workflows remain limited by their linear and static design, which restricts their ability to perform\n",
      "complex multi-step reasoning, integrate deep contextual understanding, and iteratively refine responses.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is an AI framework that enhances large language models (LLMs) by integrating real-time data retrieval from external sources—such as knowledge bases, APIs, or the web—into the response generation process. This approach addresses key limitations of standard LLMs, which often rely solely on static training data and can produce outdated information or hallucinated responses.\n",
      "\n",
      "Traditional RAG systems improve response relevance and timeliness by retrieving up-to-date, contextually relevant information before generating an answer. However, they typically follow linear, static workflows that struggle with complex tasks requiring multi-step reasoning, iterative refinement, or dynamic adaptation.\n",
      "\n",
      "To overcome these limitations, advanced variants like **Agentic RAG** have been developed. Agentic RAG embeds autonomous AI agents into the RAG pipeline, enabling capabilities such as:\n",
      "\n",
      "- **Autonomous decision-making**: Agents dynamically choose retrieval strategies based on query complexity.\n",
      "- **Iterative refinement**: Responses are improved through feedback loops.\n",
      "- **Workflow optimization**: Tasks are orchestrated adaptively, supporting sequential or collaborative multi-agent processes.\n",
      "\n",
      "While highly effective in domains like customer support, financial analytics, and personalized education—where contextual precision and adaptability are crucial—Agentic RAG also faces challenges, including coordination complexity, increased computational overhead, and scalability constraints under high load.\n",
      "\n",
      "In summary, RAG bridges the gap between static LLM knowledge and real-world dynamism, with Agentic RAG pushing the boundary further through intelligent, agent-driven workflows.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is RAG?\"\n",
    "for step in agent.stream({\"messages\": [HumanMessage(query)]}, stream_mode=\"values\"):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ac7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
